# LLM2024ã‚³ãƒ³ãƒš

äºˆé¸4ä½ : 3.83

æ±ºå‹2ä½ : 3.73835

# 1. model

**google/gemma-2-27b**

## 1.1 é¸å®šç†ç”±

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„
- llm-jpã®å‡ºåŠ›ã¯è‰¯ã„è©•ä¾¡ã‚’å—ã‘ã¦ã„ãªã„
- 4bité‡å­åŒ–ã§ä½¿ç”¨å¯èƒ½

# 2. method

## 2.1 unslothã§ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿

### 2.1.1 unslothã®æ³¨æ„ç‚¹

ä»¥ä¸‹ã®ã‚ˆã†ãªã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒ¼ãƒ‰ã ã¨unslothã®gemma2-27b-bnb-4bitãŒå‘¼ã°ã‚Œã‚‹ã€‚

â€»ã“ã‚Œã¯ã‚³ãƒ³ãƒšã®ãƒ«ãƒ¼ãƒ«ã«æŠµè§¦ã™ã‚‹æã‚Œã‚ã‚Šã€‚

```python
model_name = "google/gemma-2-27b"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)
```

ä¸€åº¦ã€4bité‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
import torch

# ãƒ¢ãƒ‡ãƒ«åã¨é‡å­åŒ–è¨­å®š
model_name = "google/gemma-2-27b"  # å…ƒã®ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®š
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)

# Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
repo_name = "tomo1222/gemma-2-27b-bf16-4bit"  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆãƒªãƒã‚¸ãƒˆãƒªå
model.push_to_hub(repo_name)
tokenizer.push_to_hub(repo_name)
```

ã“ã‚Œã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«è¡Œã†ã€‚

```python
model_name = "tomo1222/gemma-2-27b-bf16-4bit"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)
```

ãŸã ã—ã€tokenizerã®chat_templeteãŒãªã„ãŸã‚google/gemma-2-9bã®ã‚‚ã®ã‚’æµç”¨ã™ã‚‹

```python
tokenizer.chat_template = """
{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}
"""
```

## 2.2 Supervised FineTuning

### 2.2.1 å­¦ç¿’ãƒ‡ãƒ¼ã‚¿

**æ–¹é‡**

- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ç¶™æ‰¿ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãªã„ã€‚
- å€‹äººãŒä½œæˆã—ãŸã¨æ€ã‚ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãªã„ã€‚
- è³ªã®æ‚ªã„ãƒ‡ãƒ¼ã‚¿ã‚’å¯„ã›é›†ã‚ãšã«ã€è‰¯è³ªãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ç”¨ã„ã‚‹ã€‚
- åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’vLLMã§ä½œæˆã™ã‚‹å ´åˆã¯apache2.0ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ç”¨ã„ã‚‹ (çµå±€ã€Qwen2.5ã®å‡ºåŠ›ãŒå¾®å¦™ã ã£ãŸãŸã‚ç”¨ã„ãšã€‚tanukiã‚’è©¦ã™å‰ã«æ™‚é–“åˆ‡ã‚Œ)

1. llm-jp/magpie-sft-v1.0

[llm-jp/magpie-sft-v1.0 Â· Datasets at Hugging Face](https://huggingface.co/datasets/llm-jp/magpie-sft-v1.0)

- åˆæˆãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ãŸã‚ã€filterã§è³ªã®æ‚ªã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ã™ã‚‹ã€‚
- ç‰¹ã«Qwen2.5ã¯å‡ºåŠ›ãŒé€”ä¸­ã‹ã‚‰ä¸­å›½èªã«ãªã‚‹ã‚±ãƒ¼ã‚¹ãŒéå¸¸ã«å¤šã„ã€‚

```python
from datasets import concatenate_datasets, load_dataset, Dataset
from transformers import TrainingArguments, Trainer
import random
import pycld2 as cld2
import json
from collections import Counter

input_name = "llm-jp/magpie-sft-v1.0"
output_file = "filtered_magpie-sft-v1.jsonl"

# instruct
instruct_ng_list = []

# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†
def process_magpie(example):
    user_message = next(
        (conv["content"] for conv in example["conversations"] if conv["role"] == "user"), ""
    )
    assistant_response = next(
        (conv["content"] for conv in example["conversations"] if conv["role"] == "assistant"), ""
    )
    return {
        "instruction": user_message, 
        "response": assistant_response
    }

magpie = load_dataset(input_name, split="train")

magpie_processed = magpie.map(process_magpie)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµåˆ
combined_dataset = concatenate_datasets([magpie_processed])

print(len(combined_dataset))

# ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼ˆè¨±å¯ã™ã‚‹è¨€èªã‚³ãƒ¼ãƒ‰ï¼‰
whitelist = {"ja"} 
threshold = 60

# ç½®ãæ›ãˆæ–‡å­—ãƒªã‚¹ãƒˆ
replace_dict={"ã‚ˆã—mitsu":"ã‚ˆã—ã¿ã¤", "(ã—ã¿ãšã˜)":"(ãã‚ˆã¿ãšã§ã‚‰)", "ã‘ã‚“ã›ã‚“ã›ã„ã—ã‚“ã—ã£ã‹ã‚“":"ã›ã‚“ã¦ã‚“ã›ã„ã—ã‚“ã—ã£ã‹ã‚“",
              "ã€Œæœˆæ—¥ã¯ç™¾ Ø§Ù„Ø³Ù†ÙŠÙ† ÙƒØ§Ù„Ø³Ø§Ø¹Ø©ã€":"ã€Œæœˆæ—¥ã¯ç™¾ä»£ã®éå®¢ã«ã—ã¦ã€","ã€Œæœˆæ—¥ã¯ç™¾ Ø§Ù„Ø³Ù†ÙŠÙ† ÙƒØ§Ù„Ø³Ø§Ø¹Ø©ã€":"ã€Œæœˆæ—¥ã¯ç™¾ä»£ã®éå®¢ã«ã—ã¦ã€",
              "(Chikoma no tomo)":"(ã¡ãã°ã®ã¨ã‚‚)","(Taketori Monogatari)":"(ãŸã‘ã¨ã‚Šã‚‚ã®ãŒãŸã‚Š)","ã­ sake, but in this context it means Yanaka":"ã­ãã—",
              "ï¼ˆ Rokuon-ji ï¼‰":"(ãã‚“ã‹ãã˜)","ï¼ˆ Jisho-ji ï¼‰":"(ãã‚“ã‹ãã˜)","ï¼ˆ Tenryu-ji ï¼‰":"(ã¦ã‚“ã‚Šã‚…ã†ã˜)","ï¼ˆ Arashiyama ï¼‰":"ã‚ã‚‰ã—ã‚„ã¾",
              "ï¼ˆ Takeshizaka ï¼‰":"(ã¡ãã‚Šã‚“ã®ã¿ã¡)"," Weekly Shonen Jump ":"é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—","æ¾ä¸‹":"ä¸‰è±","(Regular Expressions)":"","ã‚¢ã‚¤ã‚·ãƒ†ãƒ«":"æ„›ã—ã¦ã‚‹",
              "ã‚¹ãƒ©ã‚¤cing":"ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°","(Watashi mo ikimasu.)":"","(Uchimaimasu.)":"(ã†ã‹ãŒã„ã¾ã™)","å¤‰æ›å™¨ãƒ¢ãƒ‡ãƒ«":"ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼",
              "ï¼ˆã†ã‚ãŸã ã•ã“ã‚“ã˜ï¼‰":"ï¼ˆã†ã‚ã“ã ã ã•ã“ã‚“ã˜ï¼‰","ãµã›ã‹ã‚":"ã—ãªãšãŒã‚","ã‚µãƒãƒ¼ã‚¿ì„ãƒ¬ãƒ³ãƒ€":"ã‚µãƒãƒ¼ã‚¿ã‚¿ã‚¤ãƒ ãƒ¬ãƒ³ãƒ€ãƒ¼","è¯­":"èª","ï¼ˆãã¡ï¼‰":"ï¼ˆããµï¼‰",
              "ï¼ˆã•ã•ã–ã„ï¼‰":"(ã•ã•ã¡ã¾ã)","â™ª":"","ğŸ˜¢":""
              }

# å‰Šé™¤ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ(å­˜åœ¨ã™ã‚Œã°ãƒ‡ãƒ¼ã‚¿ã‚’æ¡ç”¨ã—ãªã„)
ng_list=["ç¿»è¨³","è‹±è¨³","ï¼ˆã‚¹ãƒˆromaï¼‰",".com",
        "è¯­","å®¾","å®","é™…","é˜…","è¯»","è¿‡","è¡¥","ä¸","é¾™","å§Š","è€Œ","æ–¼","è™•","æ“š","é—œ","åˆ™","åº”","è¿œ","ç¦»","ç”µ","å…´","åŠ¨","å‘","é¦†","ç½‘","èµ„","ä»¬","ç®€","è•Š","ä»","ç»“","çº§","å…³",
        "è°ƒ","å","é’Ÿ","é”…","å¯¹","ìƒ›","ä¹¡","å­¢","ä¸“","å¹¿","ã‚","è¾“","å® ","å›","çº¦","ç‰","è½¬","é„‰","Ï","á¼","Ñ‡","å²›","ìƒ","çƒ§","ë˜","ç“Š","è½¦","è‰º","åœ³","å§¶","é š","å†°","èŒ‰","æ¶œ","æ—±",
         "çº¿","æ†","å¡Œ","æŠ¤","å¤´","è¿™","ä¸š","é¥±","é¥±","ã„","ã„ ","é¥±","ä½ ","çµ","ã‚","å‰‘","å¦","ä¸º","å„¿","è§†","é¢‘","åˆ›","ç°","ä¸œ","å›­","å«","å˜","å‹","ä¼ ","ç»Ÿ","æ±¡","é•¿","é¸Ÿ","æ´",
         "iche ", "Ich ","é ","å—½","ç±¾","è¿›","Ğ¾","ç§¯","é€‚","æ—¶",
         "ê·¸","ë ˆ","ì´","íŠ¸", "ê²Œ","ì´","ì¸ ","ë¹„","ì„","ë¬´","í˜‘","ì•ˆ","ë…•","íˆ", "ì£¼","ì„¸","ìš”","í…ƒ","ë°­","æ‰«","åœº","èŠ‚","è¥","è®¾","ç»¿","è¯—","è¯´","Ğ¿","ä¹","ì‡ ","í‡´","ä»”","é³ƒ","é¹…",
         "Gatsby","ì‡»","ä¸½","è£”","å£³","å¢™","æ ‡","Ñ","ä¸°","å±‚","æ³½","æ°”","å—","Ğ¯","æ²Ÿ","ç½—","ä¹‰","è®²","é‡‡","éªŒ","è®­","è¯¯","é”™",
         "Î¹","Îµ","á½", "ÏŒ","Ã­","Ã¨","Ã¹","Ã¬","Ğ»","Ğ½","Ğ¹","ÑŒ","Å","Ã ","Ã¡","Ã±","Ñ","Ã¶","Ã¤","Ï‡","Î¬","Ï‚","Ã³","Å«","á»‡","Ãº","Î©","Ñ‚",
         "Ù„Ø³Ù†ÙŠÙ† ÙƒØ§","Ù„","Ø³","Ø§", "Ø¹","Ø©ã€","Ø­Ù…Ø¯",
         "à¸‚","à¸­","à¸š","à¸„à¸¸","à¸“","à¸‚","à¸­","à¸š","à¸„à¸¸","à¸“","à¸¡","à¸²","à¸","à¸„","à¸£à¸±","à¸š","à¸„à¹ˆ","à¸°","Å‚","Å¼","Ã‰","Ã´","Ä‘","Ñ†","Ä","Ğ±","Â£","Ù†",
         "à®¨","à®©à¯","à®±à®¿","à¦§à¦¨à§","à¦¯","à¦¬à¦¾à¦¦","ï¼ˆalhamdulillahï¼‰","Ğ´","Ğµ","Ğ°",
         "Ã°","É™","Ğ¸","ï¿½","é“¶","å°”","âš—","ä¹ ","à¥‚","Ã¦","ì–½","â˜…","Ëˆ","Ã¼","é—´","íƒ‘","ì›©","Ã©",
         " - Konnichiwa "," - Arigatou "," - Sumimasen "," - Nandesu ka "," - Nandesu ka ",
         "ãƒ‰ã‚¤ãƒ„èª","ãƒ©ãƒ†ãƒ³èª","ä¸­å›½èª","ãƒ­ã‚·ã‚¢èª","ãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èª","ã‚¢ãƒ©ãƒ“ã‚¢èª","éŸ“å›½èª","ãƒ•ãƒ©ãƒ³ã‚¹èª","ãƒãƒ«ãƒˆã‚¬ãƒ«èª","ã‚¹ãƒšã‚¤ãƒ³èª","ã‚¤ã‚¿ãƒªã‚¢èª","ã‚¿ãƒŸãƒ«èª","ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èª","ã‚¿ã‚¤èª",
         "ãƒ™ãƒˆãƒŠãƒ èª","ã‚¸ãƒ£ãƒ¯èª","ãƒ™ãƒ³ã‚¬ãƒ«èª","ãƒˆãƒ«ã‚³èª","ã‚¦ãƒ«ãƒ‰ã‚¥èª","æœé®®èª","ãƒ•ã‚£ãƒ³ãƒ©ãƒ³ãƒ‰èª","ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³èª","ãƒãƒ³ã‚¬ãƒªãƒ¼èª","ãƒ˜ãƒ–ãƒ©ã‚¤èªï¼š","è‹±èª:",
         "```","https://","http://","<h1>","<h2>","<h3>","<h4>","<a>", "AI assistant", "è‹±èªè¨³", "ç§ã¯ã€ŒClaudeã€"]

# å‡ºç¾é »åº¦19ä»¥ä¸‹ã¯ã™ã¹ã¦é™¤å¤–ã™ã‚‹
tot_text = ""
for text in combined_dataset:
    tot_text += text["response"]+" "+text["instruction"]
    # æ–‡å­—å˜ä½ã§é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
char_counts = Counter(tot_text)
for char, count in char_counts.most_common():
    if count<=19:
        ng_list.append(str(char))
print(f"ng:{len(ng_list)}")

# ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
filtered_dataset = []
for text in combined_dataset:

    for key,value in replace_dict.items():
        if key in text["response"]:
            text["response"] = text["response"].replace(key, value)
        if key in text["instruction"]:
            text["instruction"] = text["instruction"].replace(key, value)
    
    remove = False
    for ng_word in ng_list:
        if ng_word in text["response"] or ng_word in text["instruction"]:
            remove = True
            continue
    for ng_word in instruct_ng_list:
        if ng_word in text["instruction"]:
            remove = True
            continue
    if remove:
        continue

    flag1 = False
    flag2 = False
    
    try:
        is_reliable, text_bytes_found, details = cld2.detect(text["instruction"])
        for lang in details:
            lang_name, lang_code, lang_percent,_ = lang

            
            if lang_code in whitelist and lang_percent >= threshold:
                flag1 = True
                break
        is_reliable, text_bytes_found, details = cld2.detect(text["response"])
        for lang in details:
            lang_name, lang_code, lang_percent,_ = lang

            if lang_code in whitelist and lang_percent >= threshold:
                flag2 = True
                break
        if flag1 and flag2:
            filtered_dataset.append(text)
    except Exception as e:
        print(f"Error processing text: {text}, Error: {e}")

tot_text = ""
for text in filtered_dataset:
    tot_text += text["response"]+" "+text["instruction"]
    # æ–‡å­—å˜ä½ã§é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
char_counts = Counter(tot_text)

# é »åº¦é †ã«ã‚½ãƒ¼ãƒˆã—ã¦10ä»¥ä¸‹ã®ã‚‚ã®ã‚’æ›´ã«æ¶ˆã™
ng_list = []
for char, count in char_counts.most_common():
    if count<10:
        ng_list.append(str(char))

print(f"ng:{len(ng_list)}")

filtered_dataset = [
    text for text in filtered_dataset
    if not any(ng_word in text["response"] or ng_word in text["instruction"] for ng_word in ng_list)
]

combined_dataset = Dataset.from_list(filtered_dataset)

tot_text = ""
for text in filtered_dataset:
    tot_text += text["response"]+" "+text["instruction"]
    # æ–‡å­—å˜ä½ã§é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
char_counts = Counter(tot_text)

print(len(filtered_dataset))

for char, count in char_counts.most_common():
    print(f"{char}: {count}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ JSONL å½¢å¼ã§ä¿å­˜
with open(output_file, "w", encoding="utf-8") as f:
    for data in filtered_dataset:  # filtered_dataset ã¯ãƒªã‚¹ãƒˆå½¢å¼
        json.dump(data, f, ensure_ascii=False)
        f.write('\n')

```

1. tomo1222/Japanese-QA111dataset

è‡ªèº«ã§ä½œæˆã—ãŸ111å€‹ã®QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

### 2.2.2 Trainer

- lrã¯2e-4ã§å›ºå®šã™ã‚‹ã€‚
- å­¦ç¿’ã«å¤§ããªå½±éŸ¿ã¯ãªã„ã¨è€ƒãˆã‚‰ã‚Œã‚‹target_modulesã‚„lora_dropout, optimizer, schedulerã¯å›ºå®šã¨ã™ã‚‹ã€‚
- rã¨lora_alphaã¯è©¦è¡ŒéŒ¯èª¤ã®çµæœr=64, lora_alpha=64ã‚’æ¡ç”¨ã€‚
- 1epochã§ã¯å­¦ç¿’ãŒä¸ååˆ†ã ã¨æ„Ÿã˜ãŸãŸã‚ã€è¿½åŠ ã§1pochå›ã—ãŸã€‚
- DPOã¯è¡Œã‚ãš(è‰¯è³ªãªãƒ‡ãƒ¼ã‚¿ãŒæ‰‹ã«å…¥ã‚‰ãªã‹ã£ãŸãŸã‚)

![image](https://github.com/user-attachments/assets/0338c3a3-2c9d-4d03-ae6e-c6ac489a22c0)

```python

### prompt
#### Japanese
alpaca_prompt_jp = """
ã‚ãªãŸã¯é«˜æ€§èƒ½ãªAIã§ã™ã€‚è³ªå•ã«å¯¾ã—ã¦ã€é©åˆ‡ãªå›ç­”ã‚’æ›¸ããªã•ã„ã€‚
å›ç­”ã«é–¢ä¿‚ã®ãªã„ã“ã¨ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã§ãã ã•ã„ã€‚

### è³ªå•:
{}

### å›ç­”:
{}"""
############################################
## parameters
suffix = "-jp-r64_alpha64"
project_name = "Gemma2-27b-ft" + suffix
#ã‚¹ã‚±ãƒ¼ãƒ«
lora_alpha = 64 # fix
#ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®æ¬¡å…ƒ
r = 64
#å­¦ç¿’ç‡
lr = 2e-4
# accumulation
gradient_accumulation_steps = 8
# promptã®ç¨®é¡
alpaca_prompt = alpaca_prompt_jp
# epoch
num_train_epochs = 2
############################################

wandb.init(project=project_name)

with open("filtered_dataset.jsonl","r",encoding='utf-8') as f:
    combined_dataset = [json.loads(l) for l in f.readlines()]

combined_dataset = [{"instruction":sample["instruction"],"response":sample["response"]} for sample in combined_dataset]

with open("Japanese-QA111dataset.jsonl", "r", encoding="utf-8") as f:
    dataset2=[json.loads(l) for l in f.readlines()]
for sample in dataset2:
    combined_dataset.append({"instruction":sample["input"],"response":sample["output"]})
combined_dataset = Dataset.from_list(combined_dataset)
print(len(combined_dataset))

# 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ­ãƒ¼ãƒ‰
model_name = "tomo1222/gemma-2-27b-bf16-4bit"  # Googleã®Gemmaãƒ¢ãƒ‡ãƒ«

max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

tokenizer.chat_template = """
{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}
"""

EOS_TOKEN = tokenizer.eos_token
def tokenize_function(examples):
    inputs       = examples["instruction"]
    outputs      = examples["response"]
    texts = []
    for input, output in zip(inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(input, output)
        if output:
          text += EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’é©ç”¨
tokenized_dataset = combined_dataset.map(tokenize_function, batched=True)

random.seed(45)

# 3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
model = FastLanguageModel.get_peft_model(
    model,
    r = r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = lora_alpha,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

train_args = TrainingArguments(
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = gradient_accumulation_steps,
    num_train_epochs=num_train_epochs,
    learning_rate=lr,
    lr_scheduler_type = "linear",
    do_eval=True,
    optim = "adamw_8bit",
    weight_decay=0.01,
    warmup_steps=300,
    fp16=False,
    save_steps=300,  
    save_total_limit=10,
    logging_dir="./logs", 
    logging_steps=5,
    report_to = "wandb",
    auto_find_batch_size=True,
    neftune_noise_alpha=5,
    bf16 = True,
    output_dir = "results" + suffix,
    seed = 3407
)

trainer = SFTTrainer(
    model = model,
    train_dataset = tokenized_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    tokenizer = tokenizer,
    dataset_num_proc=2,
    packing = False,
    args = train_args
)

print(trainer.model.print_trainable_parameters())

# å­¦ç¿’ã®é–‹å§‹

model.config.use_cache = False
trainer.train()
model.config.use_cache = True

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
model.save_pretrained(project_name)
tokenizer.save_pretrained(project_name)
model.push_to_hub("tomo1222/"+project_name, token = token)
tokenizer.push_to_hub("tomo1222/"+project_name, token = token)
```

### trainãƒ¡ãƒ¢

| trial | **r** | lora_alpha | epoch | result | è¿½è¨˜ |
| --- | --- | --- | --- | --- | --- |
| 1 | 16 | 16 | 1 | bad |  |
| 2 | 32 | 32 | 1 | 3 Shot : score 3.5 |  |
| 3 | 128 | 128 | 1 | ãƒ¡ãƒ¢ãƒªä¸è¶³ |  |
| 4 | 32 | 16 | 1 | 14 shots : score 3.63 |  |
| 5 | 32 | 64 | 1+2 | 14 shots : score 3.48 (è¿½åŠ ãƒ‡ãƒ¼ã‚¿100+66) |  |
| 6 | 32 | 64 | 1 | 24 shots : score 3.43 |  |
| 7 | 32 | 64 | 1 | 14 shots : score 3.54 |  |
| 8(train1.py) | 32 | 64 | 1+2epoch(77+1000) | 15 shots-ft : score 3.51 |  |
| 9
ä»¥ä¸‹åŒä¸€ãƒ¢ãƒ‡ãƒ« | 64 | 64 | 2epoch | 16RAG: score 3.52 | v2+inputOnly |
| 10 |  |  |  | 3.54 |  |
| 11 |  |  |  | 3.69 | reputation penulty 1.1 |
| 12 |  |  |  | 3.67 |  |
| 13 |  |  |  | 3.77 |  |

## 2.3 prompting

[LLMã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã¾ã¨ã‚ - Qiita](https://qiita.com/fuyu_quant/items/157086987bd1b4e52e80)

- åŠ¹æœãŒã‚ã‚Šãã†ã ã£ãŸã®ãŒã€FewShotã§äº‹ä¾‹ã‚’ç¤ºã™ã“ã¨ã€‚
- é¡ä¼¼ã‚¿ã‚¹ã‚¯ã‚’å¾ŒåŠã«é…ç½®ã—ãŸæ–¹ãŒè‰¯ã•ã’ã ã£ãŸã®ã§ã€RAGã‚’ç”¨ã„ã¦tomo1222/Japanese-QA111datasetã‹ã‚‰æ‹¾ã†ã€‚(ãŸã ã—ã€é¡ä¼¼ã‚¿ã‚¹ã‚¯ã‚’æ‹¾ãˆã¦ã¯ã„ãªã„ã®ã§åŠ¹æœã¯å¾®å¦™)

```python
from datasets import concatenate_datasets, load_dataset
from unsloth import FastLanguageModel
import random
import json

from huggingface_hub import login
from google.colab import userdata
login(userdata.get('HFtoken'))

with open("elyza-tasks-100-TV_0.jsonl","r",encoding='utf-8') as f:
    tasks = [json.loads(l) for l in f.readlines()]

model_name = "tomo1222/Gemma2-27b-ft-jp-r64_alpha64"

max_seq_length = 4096

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

# google/gemma-2-9bã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
tokenizer.chat_template = """
{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}
"""
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

dataset = load_dataset("tomo1222/Japanese-QA111dataset")
ref_tasks = list(dataset["train"])  
ref_tasks_input = [task["input"] for task in ref_tasks]

dic = {}
dic_input = {}
for i, task in enumerate(ref_tasks):
  dic[ref_tasks_input[i]] = task["output"]
  dic_input[ref_tasks_input[i]] = task["input"]

"""# 2. RAGã®ãƒ­ãƒ¼ãƒ‰"""

from ragatouille import RAGPretrainedModel
RAG = RAGPretrainedModel.from_pretrained("bclavie/JaColBERTv2")
RAG.encode(ref_tasks_input)
```

- repetation_penaltyã¯1.2ã ã¨leaderboardã‚¹ã‚³ã‚¢ãŒå®‰å®šã—ãªã„ã®ã§ã€1.1ã¾ã§ä¸‹ã’ãŸ

```python
def search_ref_input(input, k=10):
  retreived=RAG.search_encoded_docs(query=input,k=k)
  print(retreived)
  text ="è³ªå•ãƒ»æ–‡ç« ã‚’ã‚ˆãèª­ã‚“ã§ã€æ­£ç¢ºã§è¦ªåˆ‡ãªå›ç­”ã‚’æ›¸ããªã•ã„ã€‚\n"
  for data in retreived[::-1]: # inverse order
    key = data["content"]
    output = dic[key]
    input = dic_input[key]
    text+="### è³ªå•:\n"+input+"\n\n### å›ç­”:\n"+output+"\n\n\n"
  return text

"""# Prompt"""
output_data=[]

for i, task in enumerate(tasks):
  text = (
    search_ref_input(task["input"], 20)
    + "ã‚ãªãŸã¯æ—¥æœ¬èªãŒå ªèƒ½ãªå„ªç§€ãªäººé–“ã§ã™ã€‚\n"
    + "**æ–‡è„ˆ**ã‚’è¸ã¾ãˆã¦ã€æ”¹è¡Œã¨ç®‡æ¡æ›¸ãã‚’é§†ä½¿ã—ã¦ã€æ—¥æœ¬èªã§**è©³ç´°ã«**æ›¸ããªã•ã„ã€‚\n"
    + "å„ªç§€ãªäººé–“ã«ãªã‚Šãã£ã¦ã€æ¨æ¸¬ã‚’ã„ã‚Œãšã«æ ¹æ‹ ã‚’ã‚‚ã£ã¦ã‚ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚"
    + f"### è³ªå•:\n{task['input']}\n\n### å›ç­”:\n"
  )
  print(task["input"])
  inputs = tokenizer(text, return_tensors="pt").to("cuda")
  print(len(inputs['input_ids'][0]))
  output = model.generate(**inputs, max_new_tokens=1024,repetition_penalty=1.1,use_cache=True,
                          bad_words_ids = [tokenizer.encode("è³ªå•", add_special_tokens=False),
                                            tokenizer.encode("###", add_special_tokens=False),
                                            tokenizer.encode("#", add_special_tokens=False),
                                            tokenizer.encode("##", add_special_tokens=False),
                                            tokenizer.encode("---", add_special_tokens=False),
                                            tokenizer.encode("<h3>", add_special_tokens=False),
                                            tokenizer.encode("filepath", add_special_tokens=False),
                                            tokenizer.encode("> ", add_special_tokens=False),
                                          ]
                          )

  output_text = tokenizer.decode(output[0][inputs.input_ids.size(1):], skip_special_tokens=True).strip()
  print(i,output_text)
  print("---")
  output_data.append({"task_id":i,"output":output_text})
with open("output.jsonl","w",encoding="utf-8") as f:
  for result in output_data:
      json.dump(result, f, ensure_ascii=False)
      f.write('\n')
```
