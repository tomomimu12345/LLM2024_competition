from datasets import concatenate_datasets, load_dataset, Dataset
from transformers import TrainingArguments, Trainer
import random
import pycld2 as cld2
import json
from collections import Counter

input_name = "llm-jp/magpie-sft-v1.0"
output_file = "filtered_magpie-sft-v1.jsonl"

# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†
def process_magpie(example):
    user_message = next(
        (conv["content"] for conv in example["conversations"] if conv["role"] == "user"), ""
    )
    assistant_response = next(
        (conv["content"] for conv in example["conversations"] if conv["role"] == "assistant"), ""
    )
    return {
        "instruction": user_message, 
        "response": assistant_response
    }



magpie = load_dataset(input_name, split="train")

magpie_processed = magpie.map(process_magpie)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµåˆ
combined_dataset = concatenate_datasets([magpie_processed])

print(len(combined_dataset))

# ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼ˆè¨±å¯ã™ã‚‹è¨€èªã‚³ãƒ¼ãƒ‰ï¼‰
whitelist = {"ja"} 
threshold = 60

# ç½®ãæ›ãˆæ–‡å­—ãƒªã‚¹ãƒˆ
replace_dict={"ã‚ˆã—mitsu":"ã‚ˆã—ã¿ã¤", "(ã—ã¿ãšã˜)":"(ãã‚ˆã¿ãšã§ã‚‰)", "ã‘ã‚“ã›ã‚“ã›ã„ã—ã‚“ã—ã£ã‹ã‚“":"ã›ã‚“ã¦ã‚“ã›ã„ã—ã‚“ã—ã£ã‹ã‚“",
              "ã€Œæœˆæ—¥ã¯ç™¾ Ø§Ù„Ø³Ù†ÙŠÙ† ÙƒØ§Ù„Ø³Ø§Ø¹Ø©ã€":"ã€Œæœˆæ—¥ã¯ç™¾ä»£ã®éå®¢ã«ã—ã¦ã€","ã€Œæœˆæ—¥ã¯ç™¾ Ø§Ù„Ø³Ù†ÙŠÙ† ÙƒØ§Ù„Ø³Ø§Ø¹Ø©ã€":"ã€Œæœˆæ—¥ã¯ç™¾ä»£ã®éå®¢ã«ã—ã¦ã€",
              "(Chikoma no tomo)":"(ã¡ãã°ã®ã¨ã‚‚)","(Taketori Monogatari)":"(ãŸã‘ã¨ã‚Šã‚‚ã®ãŒãŸã‚Š)","ã­ sake, but in this context it means Yanaka":"ã­ãã—",
              "ï¼ˆ Rokuon-ji ï¼‰":"(ãã‚“ã‹ãã˜)","ï¼ˆ Jisho-ji ï¼‰":"(ãã‚“ã‹ãã˜)","ï¼ˆ Tenryu-ji ï¼‰":"(ã¦ã‚“ã‚Šã‚…ã†ã˜)","ï¼ˆ Arashiyama ï¼‰":"ã‚ã‚‰ã—ã‚„ã¾",
              "ï¼ˆ Takeshizaka ï¼‰":"(ã¡ãã‚Šã‚“ã®ã¿ã¡)"," Weekly Shonen Jump ":"é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—","æ¾ä¸‹":"ä¸‰è±","(Regular Expressions)":"","ã‚¢ã‚¤ã‚·ãƒ†ãƒ«":"æ„›ã—ã¦ã‚‹",
              "ã‚¹ãƒ©ã‚¤cing":"ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°","(Watashi mo ikimasu.)":"","(Uchimaimasu.)":"(ã†ã‹ãŒã„ã¾ã™)","å¤‰æ›å™¨ãƒ¢ãƒ‡ãƒ«":"ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼",
              "ï¼ˆã†ã‚ãŸã ã•ã“ã‚“ã˜ï¼‰":"ï¼ˆã†ã‚ã“ã ã ã•ã“ã‚“ã˜ï¼‰","ãµã›ã‹ã‚":"ã—ãªãšãŒã‚","ã‚µãƒãƒ¼ã‚¿ì„ãƒ¬ãƒ³ãƒ€":"ã‚µãƒãƒ¼ã‚¿ã‚¿ã‚¤ãƒ ãƒ¬ãƒ³ãƒ€ãƒ¼","è¯­":"èª","ï¼ˆãã¡ï¼‰":"ï¼ˆããµï¼‰",
              "ï¼ˆã•ã•ã–ã„ï¼‰":"(ã•ã•ã¡ã¾ã)","â™ª":"","ğŸ˜¢":""
              }

# å‰Šé™¤ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ(å­˜åœ¨ã™ã‚Œã°ãƒ‡ãƒ¼ã‚¿ã‚’æ¡ç”¨ã—ãªã„)
ng_list=["ç¿»è¨³","è‹±è¨³","ï¼ˆã‚¹ãƒˆromaï¼‰",".com",
        "è¯­","å®¾","å®","é™…","é˜…","è¯»","è¿‡","è¡¥","ä¸","é¾™","å§Š","è€Œ","æ–¼","è™•","æ“š","é—œ","åˆ™","åº”","è¿œ","ç¦»","ç”µ","å…´","åŠ¨","å‘","é¦†","ç½‘","èµ„","ä»¬","ç®€","è•Š","ä»","ç»“","çº§","å…³",
        "è°ƒ","å","é’Ÿ","é”…","å¯¹","ìƒ›","ä¹¡","å­¢","ä¸“","å¹¿","ã‚","è¾“","å® ","å›","çº¦","ç‰","è½¬","é„‰","Ï","á¼","Ñ‡","å²›","ìƒ","çƒ§","ë˜","ç“Š","è½¦","è‰º","åœ³","å§¶","é š","å†°","èŒ‰","æ¶œ","æ—±",
         "çº¿","æ†","å¡Œ","æŠ¤","å¤´","è¿™","ä¸š","é¥±","é¥±","ã„","ã„ ","é¥±","ä½ ","çµ","ã‚","å‰‘","å¦","ä¸º","å„¿","è§†","é¢‘","åˆ›","ç°","ä¸œ","å›­","å«","å˜","å‹","ä¼ ","ç»Ÿ","æ±¡","é•¿","é¸Ÿ","æ´",
         "iche ", "Ich ","é ","å—½","ç±¾","è¿›","Ğ¾","ç§¯","é€‚","æ—¶",
         "ê·¸","ë ˆ","ì´","íŠ¸", "ê²Œ","ì´","ì¸ ","ë¹„","ì„","ë¬´","í˜‘","ì•ˆ","ë…•","íˆ", "ì£¼","ì„¸","ìš”","í…ƒ","ë°­","æ‰«","åœº","èŠ‚","è¥","è®¾","ç»¿","è¯—","è¯´","Ğ¿","ä¹","ì‡ ","í‡´","ä»”","é³ƒ","é¹…",
         "Gatsby","ì‡»","ä¸½","è£”","å£³","å¢™","æ ‡","Ñ","ä¸°","å±‚","æ³½","æ°”","å—","Ğ¯","æ²Ÿ","ç½—","ä¹‰","è®²","é‡‡","éªŒ","è®­","è¯¯","é”™",
         "Î¹","Îµ","á½", "ÏŒ","Ã­","Ã¨","Ã¹","Ã¬","Ğ»","Ğ½","Ğ¹","ÑŒ","Å","Ã ","Ã¡","Ã±","Ñ","Ã¶","Ã¤","Ï‡","Î¬","Ï‚","Ã³","Å«","á»‡","Ãº","Î©","Ñ‚",
         "Ù„Ø³Ù†ÙŠÙ† ÙƒØ§","Ù„","Ø³","Ø§", "Ø¹","Ø©ã€","Ø­Ù…Ø¯",
         "à¸‚","à¸­","à¸š","à¸„à¸¸","à¸“","à¸‚","à¸­","à¸š","à¸„à¸¸","à¸“","à¸¡","à¸²","à¸","à¸„","à¸£à¸±","à¸š","à¸„à¹ˆ","à¸°","Å‚","Å¼","Ã‰","Ã´","Ä‘","Ñ†","Ä","Ğ±","Â£","Ù†",
         "à®¨","à®©à¯","à®±à®¿","à¦§à¦¨à§","à¦¯","à¦¬à¦¾à¦¦","ï¼ˆalhamdulillahï¼‰","Ğ´","Ğµ","Ğ°",
         "Ã°","É™","Ğ¸","ï¿½","é“¶","å°”","âš—","ä¹ ","à¥‚","Ã¦","ì–½","â˜…","Ëˆ","Ã¼","é—´","íƒ‘","ì›©","Ã©",
         " - Konnichiwa "," - Arigatou "," - Sumimasen "," - Nandesu ka "," - Nandesu ka ",
         "ãƒ‰ã‚¤ãƒ„èª","ãƒ©ãƒ†ãƒ³èª","ä¸­å›½èª","ãƒ­ã‚·ã‚¢èª","ãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èª","ã‚¢ãƒ©ãƒ“ã‚¢èª","éŸ“å›½èª","ãƒ•ãƒ©ãƒ³ã‚¹èª","ãƒãƒ«ãƒˆã‚¬ãƒ«èª","ã‚¹ãƒšã‚¤ãƒ³èª","ã‚¤ã‚¿ãƒªã‚¢èª","ã‚¿ãƒŸãƒ«èª","ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èª","ã‚¿ã‚¤èª",
         "ãƒ™ãƒˆãƒŠãƒ èª","ã‚¸ãƒ£ãƒ¯èª","ãƒ™ãƒ³ã‚¬ãƒ«èª","ãƒˆãƒ«ã‚³èª","ã‚¦ãƒ«ãƒ‰ã‚¥èª","æœé®®èª","ãƒ•ã‚£ãƒ³ãƒ©ãƒ³ãƒ‰èª","ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³èª","ãƒãƒ³ã‚¬ãƒªãƒ¼èª","ãƒ˜ãƒ–ãƒ©ã‚¤èªï¼š","è‹±èª:",
         "```","https://","http://","<h1>","<h2>","<h3>","<h4>","<a>", "AI assistant", "è‹±èªè¨³", "ç§ã¯ã€ŒClaudeã€"]

# å‡ºç¾é »åº¦19ä»¥ä¸‹ã¯ã™ã¹ã¦é™¤å¤–ã™ã‚‹
tot_text = ""
for text in combined_dataset:
    tot_text += text["response"]+" "+text["instruction"]
    # æ–‡å­—å˜ä½ã§é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
char_counts = Counter(tot_text)
for char, count in char_counts.most_common():
    if count<=19:
        ng_list.append(str(char))
print(f"ng:{len(ng_list)}")


# ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
filtered_dataset = []
for text in combined_dataset:

    for key,value in replace_dict.items():
        if key in text["response"]:
            text["response"] = text["response"].replace(key, value)
        if key in text["instruction"]:
            text["instruction"] = text["instruction"].replace(key, value)
    
    remove = False
    for ng_word in ng_list:
        if ng_word in text["response"] or ng_word in text["instruction"]:
            remove = True
            continue
    if remove:
        continue

    flag1 = False
    flag2 = False
    
    try:
        is_reliable, text_bytes_found, details = cld2.detect(text["instruction"])
        for lang in details:
            lang_name, lang_code, lang_percent,_ = lang

            
            if lang_code in whitelist and lang_percent >= threshold:
                flag1 = True
                break
        is_reliable, text_bytes_found, details = cld2.detect(text["response"])
        for lang in details:
            lang_name, lang_code, lang_percent,_ = lang

            if lang_code in whitelist and lang_percent >= threshold:
                flag2 = True
                break
        if flag1 and flag2:
            filtered_dataset.append(text)
    except Exception as e:
        print(f"Error processing text: {text}, Error: {e}")

tot_text = ""
for text in filtered_dataset:
    tot_text += text["response"]+" "+text["instruction"]
    # æ–‡å­—å˜ä½ã§é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
char_counts = Counter(tot_text)

# é »åº¦é †ã«ã‚½ãƒ¼ãƒˆã—ã¦10ä»¥ä¸‹ã®ã‚‚ã®ã‚’æ›´ã«æ¶ˆã™
ng_list = []
for char, count in char_counts.most_common():
    if count<10:
        ng_list.append(str(char))

print(f"ng:{len(ng_list)}")

filtered_dataset = [
    text for text in filtered_dataset
    if not any(ng_word in text["response"] or ng_word in text["instruction"] for ng_word in ng_list)
]

combined_dataset = Dataset.from_list(filtered_dataset)

tot_text = ""
for text in filtered_dataset:
    tot_text += text["response"]+" "+text["instruction"]
    # æ–‡å­—å˜ä½ã§é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
char_counts = Counter(tot_text)

print(len(filtered_dataset))

for char, count in char_counts.most_common():
    print(f"{char}: {count}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ JSONL å½¢å¼ã§ä¿å­˜
with open(output_file, "w", encoding="utf-8") as f:
    for data in filtered_dataset:  # filtered_dataset ã¯ãƒªã‚¹ãƒˆå½¢å¼
        json.dump(data, f, ensure_ascii=False)  # ensure_ascii=False for handling non-ASCII characters
        f.write('\n')
